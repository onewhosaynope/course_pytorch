{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "ru"
   },
   "source": [
    "# PyTorch Градиенты\n",
    "В этом разделе рассматривается реализация градиентного спуска в PyTorch <a href='https://pytorch.org/docs/stable/autograd.html'><strong><tt>autograd</tt></strong></a>. Инструменты включают:\n",
    "* <a href='https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward'><tt><strong>torch.autograd.backward()</strong></tt></a> \n",
    "* <a href='https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad'><tt><strong>torch.autograd.grad()</strong></tt></a>\n",
    "\n",
    "убедитесь, что понимаете следующие концепции:\n",
    "* Функции ошибок (ступенчатая и сигмовидная) \n",
    "* Быстрое кодирование\n",
    "* Максимальная вероятность \n",
    "* Перекрестная энтропия (включая перекрестную энтропию нескольких классов)\n",
    "* Обратное распространение (backprop)\n",
    "\n",
    "Дополнительные ресурсы: </h3>\n",
    "<strong>\n",
    "<a href='https://pytorch.org/docs/stable/notes/autograd.html'>PyTorch Примечания: </a></strong> <font color=black>Autograd mechanics</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "ru"
   },
   "source": [
    "## Autograd - Автоматическое дифференцирование\n",
    "В предыдущих разделах мы создавали тензоры и выполняли с ними различные операции, но мы ничего не делали для хранения последовательности операций или применения производной завершенной функции.\n",
    "\n",
    "В этом разделе мы познакомимся с концепцией <em>динамического</em> вычислительного графа, который состоит из всех объектов <em>тензор</em> в сети, а также <em>функций</em>, используемых для их создания. Обратите внимание, что только входные тензоры, которые мы создаем сами, не будут иметь связанных объектов Function.\n",
    "\n",
    "Пакет PyTorch <a href='https://pytorch.org/docs/stable/autograd.html'><strong><tt>autograd</tt></strong></a> обеспечивает автоматическую дифференциацию для всех операций с тензорами. Это возможно благодаря тому, что операции становятся атрибутами самих тензоров. Когда для атрибута тензора <tt>.requires_grad</tt> установлено значение True, он начинает отслеживать все операции с ним. Когда операция завершится, вы можете вызвать <tt>.backward()</tt>, и все градиенты будут вычислены автоматически. Градиент для тензора будет накапливаться в его атрибуте <tt>.grad</tt>.\n",
    "    \n",
    "Посмотрим на это на практике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "ru"
   },
   "source": [
    "## Обратное распространение за один шаг\n",
    "Мы начнем с применения единственной полиномиальной функции $y = f(x)$ к тензорной $x$. Затем мы сделаем обратное распределение (backprop) и напечатаем градиент $\\frac {dy} {dx}$.\n",
    "\n",
    "$\\begin{split}Function:\\quad y &= 2x^4 + x^3 + 3x^2 + 5x + 1 \\\\\n",
    "Derivative:\\quad y' &= 8x^3 + 3x^2 + 6x + 5\\end{split}$\n",
    "\n",
    "#### Шаг 1. Выполните стандартный импорт."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "ru"
   },
   "source": [
    "#### Шаг 2. Создайте тензор с <tt>requires_grad</tt>, установленным на True\n",
    "Это устанавливает вычислительное отслеживание тензора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "ru"
   },
   "source": [
    "#### Шаг 3. Определите функцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(63., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = 2*x**4 + x**3 + 3*x**2 + 5*x + 1\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "ru"
   },
   "source": [
    "Поскольку $y$ был создан в результате операции, у него есть связанная функция градиента, доступная как <tt>y.grad_fn</tt><br>\n",
    "Расчет $y$ выполняется как: <br>\n",
    "\n",
    "$\\quad y=2(2)^4+(2)^3+3(2)^2+5(2)+1 = 32+8+12+10+1 = 63$\n",
    "\n",
    "Это значение $y$, когда $x=2$.\n",
    "\n",
    "#### Шаг 4. Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "ru"
   },
   "source": [
    "#### Шаг 5. Отобразите получившийся градиент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(93.)\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "ru"
   },
   "source": [
    "Обратите внимание, что <tt>x.grad</tt> является атрибутом тензора $x$, поэтому мы не используем круглые скобки. Вычисление является результатом <br>\n",
    "\n",
    "$\\quad y'=8(2)^3+3(2)^2+6(2)+5 = 64+12+12+5 = 93$\n",
    "\n",
    "Это наклон многочлена в точке $(2,63)$.\n",
    "\n",
    "## Обратное распространение за несколько шагов\n",
    "Теперь давайте сделаем что-нибудь более сложное, включив слои $y$ и $z$ между $x$ и нашим выходным слоем $out$.\n",
    "#### 1. Создайте тензор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [3., 2., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.,2,3],[3,2,1]], requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "ru"
   },
   "source": [
    "#### 2. Создайте первый слой с помощью $y = 3x+2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.,  8., 11.],\n",
      "        [11.,  8.,  5.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = 3*x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "ru"
   },
   "source": [
    "#### 3. Создайте второй слой с помощью $z = 2y^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 50., 128., 242.],\n",
      "        [242., 128.,  50.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = 2*y**2\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "ru"
   },
   "source": [
    "#### 4. Установите на выходе среднее значение матрицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(140., grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "out = z.mean()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "ru"
   },
   "source": [
    "#### 5. Теперь выполните обратное распространение, чтобы найти градиент xпо отношению к out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10., 16., 22.],\n",
      "        [22., 16., 10.]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "ru"
   },
   "source": [
    "Вы должны увидеть матрицу 2x3. Если мы назовем последний тензор <tt>out</tt> \"$o$\", мы можем вычислить частную производную $o$ по $x_i$ следующим образом: <br>\n",
    "\n",
    "$o = \\frac {1} {6}\\sum_{i=1}^{6} z_i$<br>\n",
    "\n",
    "$z_i = 2(y_i)^2 = 2(3x_i+2)^2$<br>\n",
    "\n",
    "Чтобы решить производную от $z_i$, мы используем правило <a href='https://en.wikipedia.org/wiki/Chain_rule'>chain, </a>, где производная от $f(g(x)) = f'(g(x))g'(x)$<br>\n",
    "\n",
    "В этом случае <br>\n",
    "\n",
    "$\\begin{split} f(g(x)) &= 2(g(x))^2, \\quad &f'(g(x)) = 4g(x) \\\\\n",
    "g(x) &= 3x+2, &g'(x) = 3 \\\\\n",
    "\\frac {dz} {dx} &= 4g(x)\\times 3 &= 12(3x+2) \\end{split}$\n",
    "\n",
    "Следовательно, <br>\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i} = \\frac{1}{6}\\times 12(3x+2)$<br>\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = 2(3(1)+2) = 10$\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=2} = 2(3(2)+2) = 16$\n",
    "\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=3} = 2(3(3)+2) = 22$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "ru"
   },
   "source": [
    "## Отключить отслеживание\n",
    "Могут быть случаи, когда мы не хотим или не нуждаемся в отслеживании истории вычислений.\n",
    "\n",
    "Вы можете сбросить атрибут <tt>requires_grad</tt> тензора на месте, используя `.requires_grad_(True)` (или False) по мере необходимости.\n",
    "\n",
    "При выполнении оценок часто бывает полезно заключить набор операций в `с torch.no_grad ():`\n",
    "\n",
    "Менее используемый метод - запустить `.detach()` на тензоре, чтобы предотвратить отслеживание будущих вычислений. Это может быть удобно при клонировании тензора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "ru"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "ru",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
